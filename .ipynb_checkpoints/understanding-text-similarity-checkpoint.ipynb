{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52e1bddc",
   "metadata": {},
   "source": [
    "# Text Similarity Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b651605f",
   "metadata": {},
   "source": [
    "Problem Type: Unsupervised, Text Similarity, Info. Retrieval Problem  \n",
    "Solution Options: Lexical and Semantic  \n",
    "Analysis Options: Cosine Similariy and Soft Cosine Measure (SCM)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a4e9cc9f",
   "metadata": {},
   "source": [
    "#### Sources  \n",
    "- Activity: https://towardsdatascience.com/how-to-rank-text-content-by-semantic-similarity-4d2419a84c32\n",
    "- Cosine Similarity: https://realpython.com/build-recommendation-engine-collaborative-filtering/#how-to-find-similar-users-on-the-basis-of-ratings\n",
    "- Soft Cosine Measure: https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/soft_cosine_tutorial.ipynb\n",
    "\n",
    "#### Main Reference \n",
    "Text Analsis with Python: A Real-World Practical Approach to gaining actionable insights from your data by Dipajan Sarkar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a87b036",
   "metadata": {},
   "source": [
    "#### tfidf.rank_documents\n",
    " \n",
    "- Used the tfidf.rank_documents(search_terms: str, documents: list) function to score documents based on overlapping content\n",
    "- But did not see documentation for this anywhere "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9ef30e",
   "metadata": {},
   "source": [
    "# Steps Involved:\n",
    "\n",
    "1. Text Normalization\n",
    "    - Tokenization\n",
    "    - Lemmatization\n",
    "2. Information Retrieval\n",
    "    - Define your search term\n",
    "3. Feature Engineering\n",
    "    - TF-IDF (WordNet Lemmatizer vs Gensim)\n",
    "4. Similarity Measure\n",
    "    - Cosine Similarity vs Soft Cosine Measure (GloVe, Word2Net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d4f508",
   "metadata": {},
   "source": [
    "# Get input from user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "662a527b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type in your sentence: You are trying to eat healthier\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'You are trying to eat healthier'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Python program showing \n",
    "# a use of input()\n",
    "  \n",
    "search_terms = input(\"Type in your sentence: \")\n",
    "search_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860cec59",
   "metadata": {},
   "source": [
    "# A - Lexical Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4b3fed",
   "metadata": {},
   "source": [
    "## Simplist solution with no tokenizer/lemmatization \n",
    "\n",
    "using tfidf.rank_documents(search_terms: str, documents: list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5188c88e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.14808960350408654, 0.2461539234244219]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "\n",
    "documents = ['cars drive on the road', 'tomatoes are actually fruit', 'I would like to eat more vegetables']\n",
    "\n",
    "doc_vectors = TfidfVectorizer().fit_transform([search_terms] + documents)\n",
    "\n",
    "cosine_similarities = linear_kernel(doc_vectors[0:1], doc_vectors).flatten()\n",
    "document_scores = [item.item() for item in cosine_similarities[1:]]\n",
    "document_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2db974",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a86f9ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "08edfc7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0, 0.2371049775288231]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "# Interface lemma tokenizer from nltk with sklearn\n",
    "class LemmaTokenizer:\n",
    "    ignore_tokens = [',', '.', ';', ':', '\"', '``', \"''\", '`']\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc) if t not in self.ignore_tokens]\n",
    "\n",
    "# Lemmatize the stop words\n",
    "tokenizer=LemmaTokenizer()\n",
    "token_stop = tokenizer(' '.join(stop_words))\n",
    "\n",
    "#search_terms = 'red tomato'\n",
    "documents = ['cars drive on the road', 'tomatoes are actually fruit','I would like to eat more vegetables']\n",
    "\n",
    "# Create TF-idf model\n",
    "vectorizer = TfidfVectorizer(stop_words=token_stop, \n",
    "                              tokenizer=tokenizer)\n",
    "doc_vectors = vectorizer.fit_transform([search_terms] + documents)\n",
    "\n",
    "# Calculate similarity\n",
    "cosine_similarities = linear_kernel(doc_vectors[0:1], doc_vectors).flatten()\n",
    "document_scores = [item.item() for item in cosine_similarities[1:]]\n",
    "document_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751ab4e7",
   "metadata": {},
   "source": [
    "## Summary: Lexical Similarity (TF-idf)\n",
    "\n",
    "Pros\n",
    "- It’s fast and works well when documents are large and/or have lots of overlap.\n",
    "- It looks for exact matches, so at the very least you should use a lemmatizer to take care of the plurals.\n",
    "\n",
    "\n",
    "Limitations\n",
    "- When comparing short documents with limited-term variety — such as search queries — there is a risk that you will miss semantic relationships where there isn’t an exact word match."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57663b10",
   "metadata": {},
   "source": [
    "# B - Semantic Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b74007",
   "metadata": {},
   "source": [
    "## Preprocessing with gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2d0584af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import sub\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "query_string = 'fruit and vegetables'\n",
    "documents = ['cars drive on the road', 'tomatoes are actually fruit']\n",
    "\n",
    "stopwords = ['the', 'and', 'are', 'a']\n",
    "\n",
    "# From: https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/soft_cosine_tutorial.ipynb\n",
    "def preprocess(doc):\n",
    "    # Tokenize, clean up input document string\n",
    "    doc = sub(r'<img[^<>]+(>|$)', \" image_token \", doc)\n",
    "    doc = sub(r'<[^<>]+(>|$)', \" \", doc)\n",
    "    doc = sub(r'\\[img_assist[^]]*?\\]', \" \", doc)\n",
    "    doc = sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', \" url_token \", doc)\n",
    "    return [token for token in simple_preprocess(doc, min_len=0, max_len=float(\"inf\")) if token not in stopwords]\n",
    "\n",
    "# Preprocess the documents, including the query string\n",
    "corpus = [preprocess(document) for document in documents]\n",
    "query = preprocess(query_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b2722037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 66.0/66.0MB downloaded\n"
     ]
    }
   ],
   "source": [
    "# Load the model: this is a big file, can take a while to download and open\n",
    "import gensim.downloader as api\n",
    "glove = api.load(\"glove-wiki-gigaword-50\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cf8ab852",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.models import WordEmbeddingSimilarityIndex\n",
    "from gensim.similarities import SparseTermSimilarityMatrix\n",
    "from gensim.similarities import SoftCosineSimilarity\n",
    "\n",
    "  \n",
    "similarity_index = WordEmbeddingSimilarityIndex(glove)\n",
    "\n",
    "# Build the term dictionary, TF-idf model\n",
    "dictionary = Dictionary(corpus+[query])\n",
    "tfidf = TfidfModel(dictionary=dictionary)\n",
    "\n",
    "# Create the term similarity matrix.  \n",
    "similarity_matrix = SparseTermSimilarityMatrix(similarity_index, dictionary, tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1722169b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \t 0.688 \t tomatoes are actually fruit\n",
      "0 \t 0.000 \t cars drive on the road\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Compute Soft Cosine Measure between the query and the documents.\n",
    "# From: https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/soft_cosine_tutorial.ipynb\n",
    "query_tf = tfidf[dictionary.doc2bow(query)]\n",
    "\n",
    "index = SoftCosineSimilarity(\n",
    "            tfidf[[dictionary.doc2bow(document) for document in corpus]],\n",
    "            similarity_matrix)\n",
    "\n",
    "doc_similarity_scores = index[query_tf]\n",
    "\n",
    "# Output the sorted similarity scores and documents\n",
    "sorted_indexes = np.argsort(doc_similarity_scores)[::-1]\n",
    "for idx in sorted_indexes:\n",
    "    print(f'{idx} \\t {doc_similarity_scores[idx]:0.3f} \\t {documents[idx]}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
